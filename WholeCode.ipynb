{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706db429",
   "metadata": {},
   "source": [
    "The code was developed on google colab in combination with google drive. If you want it to work you have to save the CTW2019 hdf5 dataset in the exact same way, or change the code. The dataset can be obtained on the following url: https://data.ieeemlc.org/Ds1Detail ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9db4d",
   "metadata": {},
   "source": [
    "In the code below we introduce the dataset and improve it. Then we make functions that can be used to shape dataset into different categories described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import pathlib\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from typing import Union, List\n",
    "from pathlib import Path\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.constants\n",
    "import h5py\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "#def __enter__(self):\n",
    "#  return self\n",
    "CTW2019_PATH = Path('.').resolve().parent\n",
    "CTW_PATH = CTW2019_PATH.parent\n",
    "PROJECT_PATH = CTW_PATH.parent\n",
    "\n",
    "# Path to CTW2019 dataset\n",
    "DEFAULT_DATASET_PATH = PROJECT_PATH.joinpath('/content/drive/MyDrive/', 'CTW2019_h5')\n",
    "assert DEFAULT_DATASET_PATH.exists(), f'Missing dataset at `{DEFAULT_DATASET_PATH}`!'\n",
    "\n",
    "# Assumed antenna composition\n",
    "antennas = np.asarray([8, 1, 2])\n",
    "# Number of all antennas\n",
    "n_antennas = np.prod(antennas)\n",
    "# center frequency (1.25 GHz)\n",
    "fc = 1_250_000_000.0\n",
    "# Speed of light in vacuum\n",
    "c0 = sp.constants.speed_of_light\n",
    "# Wavelength of center frequency\n",
    "lambda0 = c0 / fc\n",
    "# Assume antenna spacing (see: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013195)\n",
    "antenna_spacing = lambda0 / 2\n",
    "# Spacing between subcarriers (see arxiv:2002.09452)\n",
    "ch_spacing = 20_000_000.0 / (1024 + 1)\n",
    "\n",
    "# Offset (correction to place antennas to the center of coordinate system)\n",
    "# Center is (3.5, -3.15, 1.8)\n",
    "#antenna_offset_correction = np.array([3.5, -3.15, 1.8])\n",
    "antenna_offset_correction = np.array([0, 0, 0], dtype=np.float32)\n",
    "\n",
    "# Offset coordinate center at antenna #0, shift it to the center of antenna\n",
    "antenna_offsets = -1 * (antennas - 1) / 2 * antenna_spacing\n",
    "antenna_offsets[-1] *= -1 # invert Z axis\n",
    "\n",
    "antenna_offsets += antenna_offset_correction\n",
    "\n",
    "# Frequencies of subcarrier (1024 subcarriers + central, where 50 edge carriers are empty/zero)\n",
    "fsub = np.concatenate([\n",
    "    np.arange(-462, 0, dtype=np.float32),\n",
    "    -np.arange(-462, 0, dtype=np.float32)[::-1],\n",
    "])\n",
    "assert fsub[0] == -462.\n",
    "assert fsub[-1] == 462.\n",
    "assert fsub[462] == 1.\n",
    "assert fsub[461] == -1.\n",
    "fsub = fc + fsub * ch_spacing \n",
    "fsub = np.expand_dims(fsub, axis=-1)\n",
    "assert fsub.shape == (924, 1)\n",
    "\n",
    "def get_tx_positions():\n",
    "    nx, ny, nz = antennas\n",
    "    x_offset, y_offset, z_offset = antenna_offsets\n",
    "    tx_pos = [\n",
    "        (dx*antenna_spacing+x_offset, dy*antenna_spacing+y_offset, -dz*antenna_spacing+z_offset)\n",
    "        for dz, dy, dx in it.product(range(nz), range(ny), range(nx))\n",
    "    ]\n",
    "    tx_pos = np.asarray(tx_pos)\n",
    "    assert tx_pos.shape == (n_antennas, 3)\n",
    "    return tx_pos\n",
    "\n",
    "def get_dataset(dataset_dir=DEFAULT_DATASET_PATH):\n",
    "    h_filename = 'h_Estimated_CTW_Train.h5'\n",
    "    h_path = dataset_dir.joinpath(h_filename)\n",
    "    assert h_path.exists(), f'{h_filename} is missing!'\n",
    "    pos_filename = 'r_Position_CTW_Train.h5'\n",
    "    pos_path = dataset_dir.joinpath(pos_filename)\n",
    "    assert pos_path.exists(), f'{pos_filename} is missing!'\n",
    "    #snr_filename = 'SNR_Est_Train.pickel'  #snr_filename = 'SNR_CTW_Train.h5'\n",
    "    #snr_path = dataset_dir.joinpath(snr_filename)\n",
    "    #assert snr_path.exists(), f'{snr_filename} is missing!'\n",
    "    kwargs = dict(mode='r', swmr=True)\n",
    "\n",
    "    with h5py.File(h_path, **kwargs) as hf:\n",
    "        h = hf['h_Estimated'][:].T\n",
    "        h = h.astype(np.float32)\n",
    "\n",
    "        # Fix #1: Correct the order of FFT components. In Data: (1 to 511, -512 to 0)\n",
    "        h = np.fft.fftshift(h, axes=2)\n",
    "\n",
    "        assert h.shape[1:] == (16, 924, 2)\n",
    "\n",
    "    #with h5py.File(snr_path, **kwargs) as hf:\n",
    "    #    snr = hf['SNR_Est'][:].T\n",
    "    #    snr = snr.astype(np.float32)\n",
    "    #    assert snr.shape[1:] == (16, )\n",
    "\n",
    "    with h5py.File(pos_path, **kwargs) as hf:\n",
    "        pos = hf['r_Position'][:].T\n",
    "        pos = pos.astype(np.float32)\n",
    "\n",
    "        # Fix #2: Correction of position data. Antenna will now be in the center.\n",
    "        offset = (3.5, -3.15, 1.8)\n",
    "        pos[:,0] -= offset[0]\n",
    "        pos[:,1] -= offset[1]\n",
    "        pos[:,2] -= offset[2]\n",
    "        assert pos.shape[1:] == (3, )\n",
    "    return h, pos\n",
    "\n",
    "def get_train_test_dataset(train_split=0.9, dataset_dir=DEFAULT_DATASET_PATH, shuffle=True, random_state=None):\n",
    "    \"\"\"CTW2019 challenge dataset is manageable and it can fit into memory.\"\"\"\n",
    "    h, pos = get_dataset(dataset_dir=dataset_dir)\n",
    "    \n",
    "    #raise DeprecationWarning\n",
    "\n",
    "    # Fix #1: remove measurements from time when device was placed on (or taken off) table\n",
    "    idx = np.argwhere(pos[:,2] < -0.515).flatten()\n",
    "    pos = pos[idx]\n",
    "    h = h[idx]\n",
    "\n",
    "    # Random generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    n_samples = len(pos)\n",
    "\n",
    "    # Shuffle or not to shuffle\n",
    "    idx = rng.permutation(n_samples) if shuffle else np.arange(n_samples)\n",
    "    split = round(len(idx) * train_split)\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "\n",
    "    # Shuffle all the dataset the same way\n",
    "    train = (h[train_idx], pos[train_idx])\n",
    "    test = (h[test_idx], pos[test_idx])\n",
    "    return train, test\n",
    "\n",
    "def random_split_dataset(dataset_dir=DEFAULT_DATASET_PATH, split=0.9, shuffle=True, memory=None, verbose=True, random_state=None):\n",
    "    _get_dataset = memory.cache(get_dataset) if memory else get_dataset\n",
    "\n",
    "    h, pos = _get_dataset(dataset_dir=dataset_dir)\n",
    "\n",
    "    # Random generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # How many samples are we dealing with\n",
    "    n_samples = len(pos)\n",
    "\n",
    "    # We will operate over indexes instead with dataset(s) directly\n",
    "    idx = rng.permutation(n_samples) if shuffle else np.arange(n_samples)\n",
    "\n",
    "    # if there is only 1 split make it as array for consistency\n",
    "    if isinstance(split, (float, int)):\n",
    "        split = (split, )\n",
    "\n",
    "    # define split indexes\n",
    "    split = [int(s * n_samples) for s in split]\n",
    "\n",
    "    # add zero and n_samples for consistency\n",
    "    split = (0, *split, n_samples)\n",
    "\n",
    "    # Finally construct batches\n",
    "    output = []\n",
    "    for i in range(1, len(split)):\n",
    "        start_idx = split[i-1]\n",
    "        end_idx = split[i]\n",
    "        batch = idx[start_idx : end_idx]\n",
    "        output.append((h[batch], pos[batch]))\n",
    "    output = list(output)\n",
    "\n",
    "    # Sanity check before exits\n",
    "    assert len(output) == (len(split) - 1), f'{len(output)} != {len(split) - 1}'\n",
    "    assert sum(map(lambda item: len(item[-1]), output)) == n_samples\n",
    "    #if verbose: print(f'Total: {n_samples}; Train: {split/n_samples*100:.2f}%; Test: {(n_samples - split)/n_samples*100:.2f}%')\n",
    "    return output\n",
    "  \n",
    "def short_edge_section_split_dataset(dataset_dir=DEFAULT_DATASET_PATH, shuffle=True, memory=None, verbose=True, random_state=None):\n",
    "    _get_dataset = memory.cache(get_dataset) if memory else get_dataset\n",
    "\n",
    "    h, pos = _get_dataset(dataset_dir=dataset_dir)\n",
    "\n",
    "    # Random generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # How many samples are we dealing with\n",
    "    n_samples = len(pos)\n",
    "\n",
    "    # We will operate over indexes instead with dataset(s) directly\n",
    "    idx = rng.permutation(n_samples) if shuffle else np.arange(n_samples)\n",
    "\n",
    "    # Reshuffle\n",
    "    h, pos = h[idx], pos[idx]\n",
    "\n",
    "    \n",
    "    x, y = pos[...,0], pos[...,1]\n",
    "    threshold = -1.0 * x + 2.0\n",
    "\n",
    "    train_idx = np.argwhere(y > threshold).flatten()\n",
    "\n",
    "    # train mask\n",
    "    mask = np.zeros(n_samples, np.bool)\n",
    "    mask[train_idx] = 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Total: {n_samples}; Train: {sum(mask)/n_samples*100:.2f}%; Test: {sum(~mask)/n_samples*100:.2f}%')\n",
    "\n",
    "    assert len(pos[mask]) > len(pos[~mask])\n",
    "\n",
    "    return (\n",
    "        (h[mask], pos[mask]),\n",
    "        (h[~mask], pos[~mask]),\n",
    "    )\n",
    "\n",
    "def long_edge_section_split_dataset(dataset_dir=DEFAULT_DATASET_PATH, shuffle=True, memory=None, verbose=True, random_state=None):\n",
    "    _get_dataset = memory.cache(get_dataset) if memory else get_dataset\n",
    "\n",
    "    h, pos = _get_dataset(dataset_dir=dataset_dir)\n",
    "\n",
    "    # Random generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # How many samples are we dealing with\n",
    "    n_samples = len(pos)\n",
    "\n",
    "    # We will operate over indexes instead with dataset(s) directly\n",
    "    idx = rng.permutation(n_samples) if shuffle else np.arange(n_samples)\n",
    "\n",
    "    # Reshuffle\n",
    "    h, pos = h[idx], pos[idx]\n",
    "\n",
    "\n",
    "    x, y = pos[...,0], pos[...,1]\n",
    "    threshold = 0.9 * x + 4.1\n",
    "\n",
    "    train_idx = np.argwhere(y < threshold).flatten()\n",
    "\n",
    "    # train mask\n",
    "    mask = np.zeros(n_samples, np.bool)\n",
    "    mask[train_idx] = 1\n",
    "\n",
    "    if verbose: print(f'Total: {n_samples}; Train: {sum(mask)/n_samples*100:.2f}%; Test: {sum(~mask)/n_samples*100:.2f}%')\n",
    "\n",
    "    assert len(pos[mask]) > len(pos[~mask])\n",
    "\n",
    "    return (\n",
    "        (h[mask], pos[mask]),\n",
    "        (h[~mask], pos[~mask]),\n",
    "    )\n",
    "\n",
    "\n",
    "def within_area_split_dataset(dataset_dir=DEFAULT_DATASET_PATH, shuffle=True, memory=None, verbose=True, random_state=None):\n",
    "    _get_dataset = memory.cache(get_dataset) if memory else get_dataset\n",
    "\n",
    "    h, pos = _get_dataset(dataset_dir=dataset_dir)\n",
    "\n",
    "    # Random generator\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # How many samples are we dealing with\n",
    "    n_samples = len(pos)\n",
    "\n",
    "    # We will operate over indexes instead with dataset(s) directly\n",
    "    idx = rng.permutation(n_samples) if shuffle else np.arange(n_samples)\n",
    "\n",
    "    # Reshuffle\n",
    "    h, pos = h[idx], pos[idx]\n",
    "\n",
    "\n",
    "    # Follow equation:\n",
    "    # r = 0.4\n",
    "    # t = np.linspace(0, 1, 360)\n",
    "    # x = r * np.cos(2*np.pi*t) + 0.8\n",
    "    # y = r * np.sin(2*np.pi*t) + 4\n",
    "    x, y = pos[...,0], pos[...,1]\n",
    "\n",
    "    radius = 0.5\n",
    "    conditions = np.sqrt((y - 4)**2 + (x - 0.8)**2) > radius\n",
    "\n",
    "    train_idx = np.argwhere(conditions).flatten()\n",
    "\n",
    "    # train mask\n",
    "    mask = np.zeros(n_samples, np.bool)\n",
    "    mask[train_idx] = 1\n",
    "\n",
    "    if verbose: print(f'Total: {n_samples}; Train: {sum(mask)/n_samples*100:.2f}%; Test: {sum(~mask)/n_samples*100:.2f}%')\n",
    "\n",
    "    assert len(pos[mask]) > len(pos[~mask])\n",
    "\n",
    "    return (\n",
    "        (h[mask], pos[mask]),\n",
    "        (h[~mask], pos[~mask]),\n",
    "    )\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d733f0",
   "metadata": {},
   "source": [
    "We create the proposed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d34dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  Conv3D,MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PirnatEco(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(32, (1, 7), strides=(1, 3), padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(1, 4))\n",
    "        self.res_1_1 = ResnetBlock(32)\n",
    "        self.res_1_2 = ResnetBlock(32)\n",
    "        self.res_2_1 = ResnetBlock(64, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(64)\n",
    "        self.res_3_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(128)\n",
    "        self.res_4_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(256)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc1 = Dense(1000, activation=tf.keras.layers.LeakyReLU(alpha=0.001))\n",
    "        self.fc = Dense(3)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:#, self.res_5_1, self.res_5_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374fd746",
   "metadata": {},
   "source": [
    "Here we create x_train, x_test, y_train, y_test datasets that will later be used to train the model. We shape it into random category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = random_split_dataset(dataset_dir=DEFAULT_DATASET_PATH, split=0.9, shuffle=True, memory=None, verbose=True, random_state=None)\n",
    "x_train, x_test, y_train, y_test = output[0][0],output[1][0],output[0][1],output[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d73d27",
   "metadata": {},
   "source": [
    "The following for loops are made to delete data that was recorded while robotic vacum cleaner was being lowered on to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421dfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(len(y_train[:,2])):\n",
    "  if not abs(y_train[:,2][_]) > 2.31:\n",
    "    y_train=np.delete(y_train, _, 0)\n",
    "    x_train=np.delete(x_train, _, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fe146",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(len(y_test[:,2])):\n",
    "  if not abs(y_test[:,2][_]) > 2.31:\n",
    "    y_test=np.delete(y_test, _, 0)\n",
    "    x_test=np.delete(x_test, _, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3c7b0",
   "metadata": {},
   "source": [
    "Finnally we train the model and test it. We also draw a graph to see how the result improves through the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "%load_ext tensorboard \n",
    "\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "def MDE(true: tf.Tensor, pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Mean euclidean Distance Error. Similar to RMSE.\"\"\"\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(true - pred), axis=-1)))\n",
    "\n",
    "model = PirnatEco(3)\n",
    "model.build((len(y_train)+len(y_test),16,924,2))\n",
    "model.summary()\n",
    "epochs = 100\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, name=\"sgd\"), loss=MDE)\n",
    "\n",
    "trainset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "testset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "trainset = trainset.shuffle(128).batch(32, drop_remainder=True)\n",
    "testset = testset.batch(4, drop_remainder=True)\n",
    "model.fit_generator(trainset, steps_per_epoch=None, epochs=epochs, verbose=1, callbacks=[tb_callback], validation_data=testset)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "prediction = model.predict(x_test)\n",
    "difference = np.subtract(y_test, prediction)\n",
    "rmse = rmse(prediction, y_test)\n",
    "print(\"average error: \", str(round(abs(difference).mean()*100, 2)),\"cm\")\n",
    "print(\"rmse: \", str(round(abs(rmse).mean()*100, 2)),\"cm\")\n",
    "print(\"Test loss (MDE) : \", str(round(abs(score)*100, 2)),\"cm\")\n",
    "print('---------------------------------------------------------------------------------------')\n",
    "difference_x=difference[:,0]\n",
    "print(\"average error_x: \", str(round(abs(difference_x).mean()*100, 2)),\"cm\")\n",
    "print('---------------------------------------------------------------------------------------')\n",
    "difference_y=difference[:,1]\n",
    "print(\"average error_y: \", str(round(abs(difference_y).mean()*100, 2)),\"cm\")\n",
    "print('---------------------------------------------------------------------------------------')\n",
    "difference_z=difference[:,2]\n",
    "print(\"average error_z: \", str(round(abs(difference_z).mean()*100, 2)),\"cm\")\n",
    "\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
